{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First\n",
    "I began by opening the excel file, noted it had 4 sheets, one of which was data and the rest were related to the challenge itself.\n",
    "\n",
    "### Second\n",
    "I chose to complete this in Python as it is becoming my language of choice, I find that employers prefer it to R (which I am generally stronger in), the visualizations are simpler to handle, and I can easily share the Jupyter Notbook with you in a state that demonstrates the outcome of my operations\n",
    "\n",
    "### Third\n",
    "I imported libraries to handle the data and began a short EDA (exploratory data analysis) to determine an idea of what I was working with and to approach the questions that I am posed in A and B.\n",
    "\n",
    "### Fourth\n",
    "Plan:\n",
    "    * Observe the frequency distribution of the time periods\n",
    "    * Find the average number of times a user uses the portal\n",
    "    * Find the portal page that is most and least frequently viewed\n",
    "    * Indicate the time periods of use and view the frequency distributions for each page\n",
    "        * Make a plot for each page's distribution, \"hued\" by week\n",
    "        * Check the mean for each period for each page and see if it \"shifts\"\n",
    "\n",
    "#### Initial EDA and transformations\n",
    "* Import data by utilizing Pandas 'read' functionality and select the appropriate sheet of the data\n",
    "* Conduct a quick view with 'head' (transposed to make it easier to read, this is personal preference)\n",
    "    * Noted the User_ID is some sort of hash, I prefer to categorize by counting numbers as strings because thy aren't meant to be operated upon (e.g. adding social security numbers doesn't make sense, so looking at them as stringed categories is more appropriate)\n",
    "    * Data is not dirty, but is fairly uninformative\n",
    "        * I will need to construct all of the relevant information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can you tell me about this data? What is in this data?\n",
    "\n",
    "## Does Portal Page usage shift week-to-week (relative toÂ when an User first used the service)?\n",
    "\n",
    "## How would you improve Portal Page engagement of Users (relative how a User uses the service)?\n",
    "\n",
    "# Findings\n",
    "\n",
    "## A\n",
    "* This data is of a longitudinal nature (panel data in econometrics) such that there are n > 1 observations transitioning over time.\n",
    "* Each observation contains (what appears to be) a hashed (encrypted by some measure) userID, an organization ID that clusters (or groups) the entities, a first use date signifiying the period of initial use of the portal, a use date signifying the period of use for the observation, and a portal page indicating the page used in the observation.\n",
    "\n",
    "\n",
    "* Data dimension = 134,950 x 5\n",
    "    * Number of users = n = 2798\n",
    "    * Number of observations = N = 134,950\n",
    "* Time period: Days (YY-MM-DD)\n",
    "    * Number of Time Periods = T = 34\n",
    "    * First time period = 2015-01-07\n",
    "    * Last time period = 2015-02-09\n",
    "    * Day with most uses: 2015-01-13 (8166)\n",
    "    * Day with least uses: 2015-02-09 (468)\n",
    "* Number of portal pages: 6\n",
    "    * Most frequently visited page: Discovery Cube (105489 visits)\n",
    "    * Least frequently visited page: Bullseye Chart (1183 visits)\n",
    "    * Most frequently visited page on first use: Discovery Cube (5017 visits)\n",
    "    * Least frequently visited page on first use: Spider Chart (61 visits)\n",
    "* Number of groups (Org_ID): 10\n",
    "    * Most populated group: BLN (695 users)\n",
    "    * Least populated group: WMT (2 users)\n",
    "    * Most frequently visits pages: EZ (29318 uses)\n",
    "    * Least frequently visits pages: WMT (1923 uses)\n",
    "    \n",
    "\n",
    "## B\n",
    "* Yes, portal page usage shifts week-to-week.\n",
    "    * As the date of first use distinguishes only the day the user first uses the protal, but doesn't distinguish which page is used first, last, or otherwise, then I can't say much about how the average usage of a particular page shifts with time relative to the user's first use of the page unless I make some broad assumptions. Given I have no background on this data, I don't feel comfortable standing behind those assumptions I would place forth - Discovery Cube is the landing page of the data, people go from the landing page generally to the Fingerprint Chart page, etc.\n",
    "\n",
    "## C\n",
    "* I have some issues making suggestions based on the lack of information in and about the inital data. To give a more robust response, I would like to know the sequence of the page use. That is, I would like to know which page a user uses first, second, and so forth, to the end of their time with the portal. \n",
    "* My initial thought was to check some form of markov process on this, finding what pages transition to others, and suggest filtering people to those different pages that have those strongest \"absorbing states\". My next thought was to see how long a user remains on a page. But, as I only know all of the pages a user uses in a day, I can only say something about the frequency of those pages - I don't have enough information to confidently state which page is most likely to improve portal engagement.\n",
    "* With that, knowing only the frequencies, I won't employ the other techniques I'd like (especially PCA), and will address only their correlations.\n",
    "* The Discovery Cube portal page is the most frequently used, week over week, followed by the Fingerprint Chart and Product Reccomendation. I don't know if that means these are defaut pages the user sees when using the portal, but if they are not, then perhaps users should be directd to use one of these three.\n",
    "    * We can see the Fingerprint Chart, Bullseye Chart, Mashboard, and Product Recommendation portal pages are highly correlated, so directing through/to those may influence the frequency as which users use either page.\n",
    "    \n",
    "# Comments\n",
    "The level of this data's information doesn't seem robust enough to address the comments completely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working through A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing\n",
    "df = pd.read_excel('/Users/mbair/Git/ConvexOptimizationWithPython/data/DSx.xlsx', sheet_name = 'Data')\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control for some naive levels of bad data\n",
    "\n",
    "# Make sure the data only appears where the fi\n",
    "df = df[df['First_Use_Date'] <= df['Use_Date']]\n",
    "\n",
    "# Remove rows with null - the data is fairly uninformative in its current state so replacing null values with anything at this point is not going to help with later analysis\n",
    "df.dropna(axis=0, how='any', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find a few specifics of the data\n",
    "minDate = df['Use_Date'].min()\n",
    "maxDate = df['Use_Date'].max()\n",
    "uniqueDate = df['Use_Date'].nunique()\n",
    "print('The time span is from {} to {} with {} periods.'.format(minDate, maxDate, uniqueDate))\n",
    "print()\n",
    "n = df['User_ID'].nunique()\n",
    "g = df['Org_ID'].nunique()\n",
    "avgN = df['User_ID'].value_counts().mean()\n",
    "print('The number of users is {}, appearing approximately {:,.2f} times, grouped under {} organizations.'.format(n, avgN, g))\n",
    "print()\n",
    "pPage = df['Portal_Page'].nunique()\n",
    "print('There are {} unique portal pages titled: {}.'.format(pPage, list(df['Portal_Page'].unique())))\n",
    "print()\n",
    "print(df['Portal_Page'].value_counts())\n",
    "print()\n",
    "print(df[df['First_Use_Date'] == df['Use_Date']]['Portal_Page'].value_counts())\n",
    "print()\n",
    "print(df.groupby('Org_ID')['User_ID'].nunique())\n",
    "print()\n",
    "print(df['Org_ID'].value_counts())\n",
    "print()\n",
    "print(df['Use_Date'].value_counts())\n",
    "print()\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working through B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add variables for manipulation and for simplicity in viewing\n",
    "\n",
    "# Use this instead of the hashed User_ID for indexing - just easier for me to view if I need to\n",
    "df['id'] = pd.Categorical((pd.factorize(df['User_ID'])[0] + 1).astype(str))\n",
    "\n",
    "df['group'] = df['Org_ID'].astype('category').cat.rename_categories(range(1, df['Org_ID'].nunique() + 1))\n",
    "df['period'] = df['Use_Date'].astype('category').cat.rename_categories(range(1, df['Use_Date'].nunique() + 1))\n",
    "df['week'] = df['Use_Date'].dt.week\n",
    "df['page'] = pd.Categorical((pd.factorize(df['Portal_Page'])[0] + 1).astype(str))\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variables for values\n",
    "\n",
    "# Total times each unique user appears in the data set\n",
    "df['totalPageUsesOverall'] = df.groupby('Portal_Page')['Portal_Page'].transform('count')\n",
    "df['totalPortalUsesByUser'] = df.groupby('User_ID')['User_ID'].transform('count')\n",
    "df['totalPortalByUserByWeek'] = df.groupby(['User_ID', 'week'])['User_ID'].transform('count')\n",
    "df['totalPageUsesByUser'] = df.groupby(['User_ID', 'Portal_Page'])['Portal_Page'].transform('count')\n",
    "df['totalPageByUserByWeek'] = df.groupby(['User_ID', 'week', 'Portal_Page'])['Portal_Page'].transform('count')\n",
    "df['totalPageUsesByUserByDay'] = df.groupby(['User_ID', 'Use_Date', 'Portal_Page'])['Portal_Page'].transform('count')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of days each unique user appears in the data set\n",
    "numDays = pd.DataFrame(df.groupby('User_ID')['Use_Date'].nunique())\n",
    "numDays.columns = ['numDaysByUser']\n",
    "df = pd.merge(df, numDays, how = 'inner', on = 'User_ID')\n",
    "\n",
    "# Total number of weeks each unique user appears in the data set\n",
    "numWeeks = pd.DataFrame(df.groupby('User_ID')['week'].nunique())\n",
    "numWeeks.columns = ['numWeeksByUser']\n",
    "df = pd.merge(df, numWeeks, how = 'inner', on = 'User_ID')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relevant Averages\n",
    "\n",
    "# Average daily portal usage for each unique user\n",
    "df['avgPortalUsesByUser'] = df['totalPortalUsesByUser'] / df['numDaysByUser']\n",
    "# Average weekly portal usage for each unique user\n",
    "df['avgPortalUsesByUserByWeek'] = df['totalPortalByUserByWeek'] / df['numWeeksByUser']\n",
    "\n",
    "# Average page usage for each unique user\n",
    "df['avgPageOverall'] = df['totalPageUsesByUser'] / df['numDaysByUser']\n",
    "\n",
    "# Average weekly page usage for each unique user\n",
    "df['avgPageWeekly'] = df['totalPageByUserByWeek'] / df['numWeeksByUser']\n",
    "\n",
    "# Like to find the average number of times a protal page is used on average by week given the total number of times users access a particular page ina aparticular week\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.pivot_table(values = 'totalPageByUserByWeek', index = 'week', columns = 'Portal_Page')\n",
    "df1.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See correlations of pages\n",
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(round(df1.corr(),2), cmap = 'coolwarm', linecolor = 'white', annot = True, linewidths = 3)\n",
    "t = f.suptitle('Portal Page Correlation Heatmap', fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bullseye and Fingerprint Chart are highly correlated (0.89)\n",
    "* Discovery Cube and Bullseye Chart are highly correlated (0.84)\n",
    "* Mashboard and Bullseye Chart are highly correlated (0.88)\n",
    "* Product Recommendation and Mashboard are highly correlated (0.86)\n",
    "* Spider and Fingerprint Chart are highly correlated (0.71)\n",
    "* Spider Chart and Product Recommendation are negatively correlated (-0.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.lineplot(data=df1)\n",
    "f.suptitle('Portal Page Average Usage By Week', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "for page in list(df1.columns):\n",
    "    sns.kdeplot(df1[page], shade = True)\n",
    "    sns.rugplot(df1[page])\n",
    "f.suptitle('Distribution of Average Page Usage by Users Over Weeks', fontsize = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working through C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See correlations of pages\n",
    "f, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(round(df1.corr(),2), cmap = 'coolwarm', linecolor = 'white', annot = True, linewidths = 3)\n",
    "f.suptitle('Portal Page Correlation Heatmap', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.clustermap(df1.corr(), cmap = 'coolwarm', annot = True, standard_scale = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
